
---
title: "Exercise 1 - MSA"
author: "Matteo Bandiera - Samuele Fonio - Luca Macis"
output: pdf_document
---

# Exercise 1

## Point 1.1

Compute the sample mean and the correlation matrix and comment on the correlations.

Let's start by commenting on the sample mean of the variables. Here we find a summary of the data 


```{r include=FALSE}
usair<-read.table("data/usair.txt",header=TRUE) 

usair
head(usair)
dim(usair)

library(tidyverse)
usair1<-select(usair,Neg.Temp:Days)
usair1

n<-dim(usair1)[1]
p<-dim(usair1)[2]
```


```{r echo=FALSE, message=FALSE, warning=FALSE}

library(kableExtra)
kbl(summary(usair1),booktabs = T)
```

And now let's see the plots of the sample mean:

```{r echo=FALSE, fig.dim=c(6,3)}
bar.x<-colMeans(usair1)

par(mfrow=c(1,2))
plot(bar.x[c(2,3)],type="h",lwd=3,ylim=c(0,max(bar.x[c(2,3)])),axes=F,xlab="",ylab="mean",main = "Us air pollution")

box()
axis(2)
# axis(1,at=1:6,labels=names(mineral),cex.axis=0.8)
axis(1,at=1:2,labels=names(usair1)[c(2,3)],las=3)


plot(bar.x[-c(2,3)],type="h",lwd=3,ylim=c(min(bar.x[-c(2,3)]),max(bar.x[-c(2,3)])),axes=F,xlab="",ylab="mean",main = "Us air pollution")

box()
axis(2)
# axis(1,at=1:6,labels=names(mineral),cex.axis=0.8)
axis(1,at=1:4,labels=names(usair1)[-c(2,3)],las=3)

```

For a better view we decided to distinguish between the two kind of data that we are treating. In fact as we can see Manuf and Pop are something related to the humans, and have bigger scales. We will refer to them as "human ecology". While the other variables deal with the nature. We will refer to them as "climate".  
As we can see the scales are so different, and we have to remember that are all different kind of measures. The only note to take into account is the negative mean of the Neg. Temp, that was predictable since the definition of this variable.  
In order to present the correlation matrix in a suitable way we used a specific package, that presented us this output.

```{r echo=FALSE, warning=FALSE}
library(ggcorrplot)

R<-cor(usair1)
ggcorrplot(R,lab=T,type = "upper",title="Correlation matrix for Usair",ggtheme = ggplot2::theme_gray())

```


The only strong correlation between the variables is present between Pop and Manuf. This was predictable for what they represent. The second stronger correlation in between Days and precipitations, which was also predictable always by their definitions. It was expected a weak correlation between the human ecology and climate, because the presence of population does not effect directly the climate behavior. It's also interesting noting the negative correlation of the precipitation with respect to the negative temperature that is in any case justified by the definition of the variables.

\newpage



## Point 1.2

In point 1.2 we are going to study the outliers starting from the boxplot.

```{r echo=FALSE, warning=FALSE, fig.dim=c(6,4)}
l=list()      #This is the list in which are stored the indexes of the outliers

par(mfrow=c(1,2))
for (j in 1:p) {
  x=usair1[,j]
  main.lab<-names(usair1)[j]
  
  boxplot(x,main=main.lab)
  
  values<-boxplot.stats(x)
  values$stats
  
  ind<-which(x<values$stats[1] | x>values$stats[5])
  ind
  
  l[[j]]<-ind
  
  if(j==2)
  {
    text(rep(1,3),x[ind[c(1,3,4)]],labels=as.character(ind[c(1,3,4)]),pos=4,cex=0.75,offset=0.3)
    text(c(1),x[ind[2]],labels=as.character(ind[2]),pos=2,cex=0.75,offset=0.3)
    points(rep(1,length(ind)),x[ind],col="red",pch=16)
  }
  
  else if(j==5)
  {
    text(c(1),x[ind[1]],labels=as.character(ind[1]),pos=2,cex=0.75,offset=0.3)
    text(c(1),x[ind[2]],labels=as.character(ind[2]),pos=4,cex=0.75,offset=0.3)
    points(rep(1,2),x[ind],col="red",pch=16)
  }
  
  else if(length(ind)==0)
  {
    next
  }
  
  else
  {
  text(rep(1,length(ind)),x[ind],labels=as.character(ind),pos=4,cex=0.75,offset=0.3)
  
  points(rep(1,length(ind)),x[ind],col="red",pch=16)
  }
  
  
}
```

These are the outliers detected from the boxplots. We stored them in a list.  
In this way we will be able to compare them in future. For the moment let's just comment a little bit on this output.  
The manuf and pop variables have almost the same outliers, which is not surprising since they are strongly correlated.
The wind variable has no outliers, while we can comment about the observation 23 as an outlier for days and precip. We can see from the correlation matrix that the correlation between these two variables is the second strongest among all, so we could expect a common outlier.  
So for the moment we have to recall that we have the indexes of the observations detected in a list and that these will be useful in the future analysis. But for doing further analysis of the outliers, we have to check the normality of our variables.

\newpage

## Point 1.3

In this point we have to discuss the normality. We will use two important tools: histograms and qqplots.


```{r echo=FALSE, fig.dim=c(6,4)}
par(mfrow=c(1,2))
for (j in 1:p) {
  x=usair1[,j]
  main.lab<-names(usair1)[j]
  
  hist(x,probability = T,main=main.lab)
  lines(density(x),col="red",lwd=3)
  
  qqnorm(x,main=main.lab,pch=16)
  qqline(x,col="red")
}
```

As we can see there seems to be no problem of normality for the "natural" variables: the histogram shows a normality density pattern and the quantiles seem to not behave in a strange way. This is not true for the human ecology. Let's study in particular the qqplots. We can color the outliers found in point 1.2 and add a text to recognize them.

```{r echo=FALSE,fig.dim=c(6,4)}

usair_std<-scale(usair1,scale=T)     #Standardized values
usair_std<-as.data.frame(usair_std)

par(mfrow=c(1,2))
for (j in 2:3) {
  x=usair1[,j]
  main.lab<-names(usair1)[j]
  
  #hist(x,probability = T,main=main.lab)
  #lines(density(x),col="red",lwd=3)
  
  color=rep("black",41)
  color[l[[j]]]<-"red"
  
  qqnorm(x,main=main.lab,pch=16,col=color)
  

  text(qnorm(ppoints(x))[seq(n-length(l[[j]])+1,n)],sort(x)[seq(n-length(l[[j]])+1,n)],as.character(l[[j]]),pos=2,cex=0.75,offset=0.4)
  
  
  qqline(x,col="red")
}
```

As we can see the highlighted observations are the outliers detected in point 1.2. So the question is: if they don't behave as the sample, may them affect also the normality? So in order to answer we can try to cut them off and comment:

```{r echo=FALSE, fig.dim=c(6,4)}
par(mfrow=c(1,2))
for (j in 2:3) {
  x=usair1[,j]
  main.lab<-names(usair1)[j]
  
  x<-x[-l[[j]]]
  
  color=rep("black",length(x))
  #color[l[[j]]]<-"red"
  
  qqnorm(x,main=main.lab,pch=16,col=color)
  

  #text(qnorm(ppoints(x))[seq(n-length(l[[j]])+1,n)],sort(x)[seq(n-length(l[[j]])+1,n)],as.character(l[[j]]),pos=2,cex=0.75,offset=0.4)
  
  
  qqline(x,col="red")
}

```

Without those observation the sample shows a normal behavior. This means that the outliers found in point 1.2 affect also the normality, hiding it. Anyway this problem could be solved doing a bivariate normal analysis.

```{r echo=FALSE}
S<-var(usair1)
j<-2
k<-3
d=mahalanobis(usair1[,c(j,k)],center = bar.x[c(j,k)],cov=S[c(j,k),c(j,k)])


#order(d)
color=rep("black",41)

plot(qchisq(ppoints(d),df=2),sort(d),col=color,pch=16, main="Chisq Q-Q plot of Mahalanobis distance for Manuf vs Pop",xlab="Theoretical Quantiles",ylab="Sample Quantiles")
abline(0,1,col="red")
text(qchisq(ppoints(d),df=2),sort(d),labels = order(d),pos=2,cex = 0.75,offset = 0.3)
```

This shows us that the bivariate behavior of (Manuf,Pop) agrees with a normal behavior of the single variables. So we can conclude that they cannot be considered normal in the univariate case, even if the observations that make them non-normal are outliers, but this does not affect the multivariate analysis.  
Furthermore, observation 11 will be interesting to study.  
Anyway, we can do another step in order to detect the outliers. In fact we can see which observations alter the normality with the following plots.

```{r echo=FALSE, fig.dim=c(8,3)}
plot(rep(0,1),col="white",axes = F,xlab = "",ylab = "")
legend(0.9,0.25, legend=c("0.95 quantile", "0.975 quantile","0.99 quantile"),
       col=c("yellow", "blue","green"), lty=2, cex=0.8)
```


```{r echo=FALSE, fig.dim=c(6,4)}

l1<-list()

par(mfrow=c(1,2))
for (j in 1:p) {
  y<-usair_std[,j]                     #First standardized variable
  main.lab<-names(usair_std)[j]
  
  bar.y<-mean(y)                       #Mean of the standardized variable
  
  d=rep(0,41)                          #Now we create the vector
  
  for (i in 1:41){
    d[i]<-y[i]-bar.y
  }
  
  #d
  #Òind0.95<-which(abs(usair_std[,j])>qnorm((n-0.5)/n),arr.ind=T)
  ind0.975<-which(abs(usair_std[,j])>qnorm((n-0.25)/n),arr.ind=T)
  ind0.99<-which(abs(usair_std[,j])>qnorm((n-0.1)/n),arr.ind=T)
  
  l1[[j]]<-ind0.975
  
  color<-rep("black",41)
  #color[ind0.95]<-"green"
  color[ind0.975]<-"blue"
  color[ind0.99]<-"yellow"
  color[l[[j]]]<-"red"
  
  plot(abs(d),pch=16,main=main.lab,ylab = "distance",col=color,ylim = c(0,max(abs(d))))                 #In valore assoluto perchè è la distanza
  
  if (length(ind0.975)==0) next
  
  #else{
  #text(ind0.975,abs(d)[ind0.975],labels = ind0.975,pos = 4,cex = 0.75,offset = 0.3)
  #}
  #text(qnorm(ppoints(x)),sort(x),as.character(order(x)),pos=4,cex=0.75,offset=0.4)
  abline(h=qnorm((n-0.5)/n),lty=2,col="green")
  abline(h=qnorm((n-0.25)/n),lty=2,col="blue")
  abline(h=qnorm((n-0.1)/n),lty=2,col="yellow")
}
```

We made these plot standardizing the sample, and then comparing the distance from the mean with the normal quantiles of order 0.95 (green), 0.975 (blue) and 0.99 (yellow) weighted with the sample dimension. We highlighted in red the outliers detected in point 1.2.  
First of all we can notice that some red points are over the 0.99 percentile, which means that they are far from the mean in a sample and theoretical way. In general we can say that the behavior is not strange, since only the outliers already detected are, sometimes, over the 0.99 (or less) quantiles. This shows us that our variables do not disagree with a normal pattern. For the Human ecology we can notice that our outliers have a great weight also in this analysis.  
The wind behavior seems to be the most standard in this sense: it did not have any outlier and the quantiles are not strange.






\newpage

## Point 1.4

We can use scatterplots to detect the outlier found at point 1.2. In particular we can study case by case the following plots, in which we colored with a scale of colors the outliers of point 1.2:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,7)}
color<-rep("black",41)
rain<-c("red","blue","green","yellow","orange","light blue")
for (j in 1:p) {
  color[l[[j]]]<-rain[j]
}
pairs(usair1,pch=16,cex=1.5,lower.panel=NULL, col=color)
```


This image is not so clear, but can give us an idea of what is happening: we are matching the variables two by two and studying their relation. We'll see only the most important and emblematic cases. In the following we are coloring the outliers of point 1.2, in particular red is referred to the second variable and blue to the first. Furthermore we add the ellipse of level 0.95 (weighted with the sample size) just to see what the points do.

```{r echo=FALSE, message=FALSE, warning=FALSE}

S<-cov(usair1)
library(ellipse)

par(mfrow=c(1,2))
j<-1
k<-2
color1=rep("black",41)
    color1[l[[j]]]<-"red"
    color1[l[[k]]]<-"blue"
    x1<-usair1[,j]
    x2<-usair1[,k]
    main.lab1<-names(usair1)[j]
    main.lab2<-names(usair1)[k]
    plot(x1~x2,pch=16,cex=1,col=color1,main=paste (main.lab1,"vs",main.lab2),xlab=main.lab2,ylab = main.lab1,
                xlim=c(min(x2),max(x2)),ylim=c(min(x1),max(x1)))
    lines(ellipse(x=S[c(k,j),c(k,j)],centre=bar.x[c(k,j)],level=(n-0.5)/n ))
    
j<-1
k<-3
color1=rep("black",41)
    color1[l[[j]]]<-"red"
    color1[l[[k]]]<-"blue"
    x1<-usair1[,j]
    x2<-usair1[,k]
    main.lab1<-names(usair1)[j]
    main.lab2<-names(usair1)[k]
    plot(x1~x2,pch=16,cex=1,col=color1,main=paste (main.lab1,"vs",main.lab2),xlab=main.lab2,ylab = main.lab1,
                xlim=c(min(x2),max(x2)),ylim=c(min(x1),max(x1)))
    lines(ellipse(x=S[c(k,j),c(k,j)],centre=bar.x[c(k,j)],level=(n-0.5)/n))
    
```

We decided to put this graphs because they are emblematic: the outliers are clearly out of the range if we consider just the single axes,and as we can see they are all inside the ellipse of level 0.95. We'll check upper levels later on. For the moment it's enough to note that the univariate outliers are detectable from these plots. Furthermore recall that Manuf and Pop were strongly correlated, so we expect almost the same behavior when we compare them to any variable, as in this case. In fact these two graphs seem to be very similar. However from this plot is also possible to see the weak correlation between Neg. Temp and bot Manuf and Pop: the points does not concentrate around any line.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,3)}
j<-1
k<-4
color1=rep("black",41)
    color1[l[[j]]]<-"red"
    color1[l1[[k]]]<-"dark green"
    x1<-usair1[,j]
    x2<-usair1[,k]
    main.lab1<-names(usair1)[j]
    main.lab2<-names(usair1)[k]
    plot(x1~x2,pch=16,cex=1,col=color1,main=paste (main.lab1,"vs",main.lab2),xlab=main.lab2,ylab = main.lab1,
                xlim=c(min(x2),max(x2)),ylim=c(min(x1),max(x1)))
    lines(ellipse(x=S[c(k,j),c(k,j)],centre=bar.x[c(k,j)],level=(n-0.5)/n))
```

For this plot we had just one observation of point 1.2. Since also from previous analysis the behavior seem regular, we could expect that all the points fall inside the ellipse. As we can see there isn't a strong line shape, as there is not a strong correlation, but the points seem to be more ordered than the previous two graphs.   

```{r echo=FALSE, message=FALSE, warning=FALSE,fig.dim=c(6,4)}
j<-1
k<-5
color1=rep("black",41)
    color1[l[[j]]]<-"red"
    color1[l[[k]]]<-"blue"
    x1<-usair1[,j]
    x2<-usair1[,k]
    main.lab1<-names(usair1)[j]
    main.lab2<-names(usair1)[k]
    plot(x1~x2,pch=16,cex=1,col=color1,main=paste (main.lab1,"vs",main.lab2),xlab=main.lab2,ylab = main.lab1,
                xlim=c(min(x2),max(x2)),ylim=c(min(x1),max(x1)))
    lines(ellipse(x=S[c(k,j),c(k,j)],centre=bar.x[c(k,j)],level=(n-0.5)/n))
```

This plot is interesting since we can recall that the correlation between these two variables was negative. As a consequence the points are quite concentrated around a line with negative slope. The outliers found in point 1.2 are always present.     

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,3.7)}
j<-2
k<-3
color1=rep("black",41)
    color1[l[[j]]]<-"red"
    color1[l[[k]]]<-"blue"
    x1<-usair1[,j]
    x2<-usair1[,k]
    main.lab1<-names(usair1)[j]
    main.lab2<-names(usair1)[k]
    plot(x1~x2,pch=16,cex=1,col=color1,main=paste (main.lab1,"vs",main.lab2),xlab=main.lab2,ylab = main.lab1,
                xlim=c(min(x2),max(x2)),ylim=c(min(x1),max(x1)))
    lines(ellipse(x=S[c(k,j),c(k,j)],centre=bar.x[c(k,j)],level=(n-0.5)/n))
```


This plot is interesting since it's clear that the correlation between these two variables is strong. As we can see the outliers (those in common are in blue) are far from the other observations, as they are univariate outliers. Probably that point so far from the others is observation 11 that we saw in the QQplot of the Mahalanobis distance for these two variables. We can check it just by watching the data:

```{r}
usair1[11,]
```

As we expected it's observation 11, as we can see from the values of Manuf and Pop.

*Note*: All the comparison between a variable and Manuf/Pop have the same pattern that we have already seen, so we will neglect further comparison with these two variables.  
  
```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,3.7)}
j<-5
k<-6
color1=rep("black",41)
    color1[l[[j]]]<-"red"
    color1[l[[k]]]<-"blue"
    x1<-usair1[,j]
    x2<-usair1[,k]
    main.lab1<-names(usair1)[j]
    main.lab2<-names(usair1)[k]
    plot(x1~x2,pch=16,cex=1,col=color1,main=paste (main.lab1,"vs",main.lab2),xlab=main.lab2,ylab = main.lab1,
                xlim=c(min(x2),max(x2)),ylim=c(min(x1),max(x1)))
    lines(ellipse(x=S[c(k,j),c(k,j)],centre=bar.x[c(k,j)],level=(n-0.5)/n))
```

We put this last plot just to see a better line shape since the correlation between Precip and Days is the second biggest.  
So in general it's easy to detect the previous outliers from these plots, it's enough to restrict to one axis and see which points are far from the other observations. Sometimes this is not clear because there can be points colored that are very near other observations but we found them anyway in our point 1.2. This means that in a bivariate sense their behavior is not strange.  

Now we can also study the Mahalanobis distance (for bivariate case) to detect possible hidden outliers. We already saw something drawing the ellipses in the previous plots, but we can do more looking at the Mahalanobis distance. However we have to remember that the Mahalanobis distance is standardized with respect to the standard deviation.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,2)}
S=var(usair1)
par(mfrow=c(1,3))
for (j in 1:p) {
  for (k in j:p) {
    if(j==k) next
    
    d=mahalanobis(usair1[,c(j,k)],center = bar.x[c(j,k)],cov=S[c(j,k),c(j,k)])
    color=rep("black",41)
    
    
    color[l[[j]]]<-"red"
    color[l[[k]]]<-"blue"
    main.lab1<-names(usair1)[j]
    main.lab2<-names(usair1)[k]
    plot(d,pch=16,ylim=c(min(d),max(d)+5), main=paste (main.lab1,"vs",main.lab2),col=color)
    
    qchisq((n-0.5)/n,df=2)
    
    abline(h=qchisq((n-0.5)/n,df=2),lty=2)
    
    abline(h=qchisq((n-0.25)/n,df=2),lty=2)
    
    abline(h=qchisq((n-0.1)/n,df=2),lty=2)
    
  }
}
```

As we can see there are no new outliers to detect.  This means that standardizing the data they don't represent any strange behavior. Indeed after standardization they are all below level 0.95.    

To conclude: points detected in point 1.2 are detectable from these scatterplots if we focus on each axis singularly. If we want to study the bivariate case, we can do it but, as we saw, there is no significant result to highlight.

\newpage


## Point 1.5 and 1.6

Observe that we already did a sort of multivariate normality analysis, but in a bivariate sense. And everything seem to work quite well.We now move on the multivariate (p-dimensional) case:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,4)}
S<-cov(usair1)

d=mahalanobis(usair1,center = bar.x,cov=S)

color=rep("black",41)

ind=which(sort(d)>11.338810)  #Observations on the sorted vector

ind1=which(d>11.338810)      #Observations on the sample
#color[ind]="red"

#d[c(9,1,11)]==sort(d)[c(39,40,41)]

plot(qchisq(ppoints(d),df=6),sort(d),col=color,pch=16, main="Multivariate QQ-chisquared plot of Mahalanobis distance",ylab="sorted distance",xlab = "Theoretical quantile")
abline(0,1,col="red")
text(qchisq(ppoints(d),df=6)[c(39,40,41)],sort(d)[c(39,40,41)],labels = as.character(ind1),pos=4,cex = 0.75,offset = 0.3)

```

As we can see the behavior is quite linear and so we can assume the singular normality behavior. In fact all the points are near the line bisecting the first quadrant, except for those three observations that are our new possible multivariate outliers. This plot confirm also the good multivariate behavior of the Mahalanobis distance that, theoretically should behave as a chisquared distribution.  
It's quite interesting noting that there is also observation 9 among the new possible outliers. Observation 9 was an outlier just for Neg. Temp, but in a multivariate sense its importance is big.

We can also compare the Mahalanobis distance for detecting them:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,4)}

d=mahalanobis(usair1,center = bar.x,cov=S)

color=rep("black",41)
ind1=which(d>11.338810)
color[ind1]<-"red"
plot(d,col=color,pch=16,ylim=c(0,30), ylab="distance",main="Mahalanobis distance")
text(ind1,d[ind1],labels = ind1,pos = 4,cex = 0.75,offset = 0.3)


abline(h=qchisq((n-0.5)/n,df=p),lty=2)
abline(h=qchisq((n-0.25)/n,df=p),lty=2)
abline(h=qchisq((n-0.1)/n,df=p),lty=2)
```

As we can see observation 11 is a real outlier, while with respect to the 0.95 quantile observation 9 is not.  
So from these plots we can conclude also our analysis about multivariate outliers. In fact as we know a scatter plot of six variables is not possible to perform, but from these last two plots we can see that there are observations number 1,9 and 11 that don't behave as wanted.   
In particular, watching at the Mahalanobis distance plot, we can conclude that observation number 9 is not a very multivariate outlier. This was predictable, since as we stated before observation 9 is an outlier only for the first variable.  
Observation 1 is under level 99%. However its behavior is on the edge. In fact also its bivariate analysis shows that in any analysis that involved precip and/or days (the variables for which it was a univariate outlier)  it was very close to the quantiles, and in some cases it also went over the 99% level. If we don't consider it officially an outlier we can anyway consider that it's a point of particular interest.   
Observation 11 is well known since it's been detected in all previous analysis. The over 99% level with respect to the chisquared quantile confirms that it is clearly a multivariate outlier. Furthermore we can recall the fact that it was a univariate outlier for Manuf and pop, but also the only bivariate outlier for these ones and for all bivariate comparison involving manuf and/or Pop.

\newpage





# Exercise 2

In this exercise we had to perform PCA on a simulated sample $X=(X_1,X_2,X_3)$ distributed as $N_3(\mu,\Sigma)$, with :
$$ \mu= [1 \ \ \ \ -1 \ \ \ \ 2]^t$$
$$\Sigma=\left(\begin{array}{ccc} 
1 & \rho & 0\\
\rho  & 1 & \rho \\
0 & \rho & 1
\end{array}\right)$$


Let's first of all detect the eigenvalues and eigenvector for studying the PCs:
Since the problem is symmetric and we have to order the eigenvalues, for now we can assume $\rho>0$. In fact the case $\rho<0$ present the same eigenvalues:  

\begin{align*}
\lambda_1 &=1+\sqrt2 \rho \\
    \lambda_2 &=1 \\
    \lambda_3 &=1-\sqrt2 \rho
\end{align*}

While the eigenvectors are:

\begin{align*}
a_1 &=(1, \sqrt 2 ,1) \\
    a_2 &=(1, 0 ,-1) \\
    a_3 &=(1, -\sqrt 2 ,1)
\end{align*}

To see for which $\rho$ the PC1 and PC2 are enough to describe the data we can check that

$$ \frac{\lambda_1+\lambda_2}{\lambda_1+\lambda_2+\lambda_3}\geq0.80$$

and we get $|\rho|\geq0.2828$.  
The meaning of the eigenvectors is the contribution of the original variables to each PC. It can be easily seen that $a_1,a_2$ and $a_3$ are perpendicular, But they are not unitary normed. To have this we should divide by their euclidean norm.  
If we standardize them and analyse each contribution to the first two PCs we get that the contribution of the first variable to the first PC1 is less than the one for PC2. The second variable contributes only to the PC1, since $a_2^2=0$. The third variable contributes in a way similar to the first variable, but for the PC2 the contribution is negative.  

We can easily see that $Z$, as defined, is a linear transformation of $X$ with a particular matrix $A \in \Re^{(2,3)}$:  
$$Z=AX$$
$$A=\left(\begin{array}{ccc} 
1 & -1 & 0\\
0  & 1 & -1 \\
\end{array}\right)$$

So we get that:$Z \sim N_2(A\mu,A\Sigma A^t)= N_2(\mu_z,\Sigma_z)$, with:

$$\mu_z=(2,-3)$$

$$\Sigma_z=\left(\begin{array}{ccc} 
-2\rho+2 & 2\rho-1 \\
2\rho-1 & -2\rho+2
\end{array}\right)$$

If $\rho=-2/3$:  
$$\Sigma_z=\left(\begin{array}{ccc} 
\frac{10}{3} & -\frac{7}{3} \\
-\frac{7}{3} & \frac{10}{3}
\end{array}\right)$$

So now we can perform our simulation and show the ellipse. This Ellipse will be quite large since the determinant is $|\Sigma_z|=2$. Furthermore from the sign of the covariance we can see that the form will be:

```{r echo=FALSE, fig.dim=c(6,3), message=FALSE, warning=FALSE}

library(MASS)
rho=-2/3
mu_z=c(2,-3)
sigma_z=matrix(c(-2*rho+2,2*rho-1,2*rho-1,-2*rho+2),nrow = 2)
#sigma_z
Z<-mvrnorm(n=100,mu=mu_z,Sigma=sigma_z)
#Z
colnames(Z)=c("x1","x2")

plot(x2~x1, data=Z, xlab="Z1", ylab="Z2", asp=1,
     main="Plot of Z with rho=-2/3", pch=16,
     xlim=c(min(x1)-1,max(x1)+1), ylim=c(min(x2)-1,max(x2)+1))                           

#round(cov(Z),3)
# sample covariance matrix
#round(cor(Z),3)
# sample correlation matrix

pca.norm<- prcomp(Z)
# Principal Component Analysis
#pca.norm
#eigen(cov(Z))



#summary(pca.norm)

library(ellipse)

a1<-pca.norm$rotation[,1];a2<-pca.norm$rotation[,2]
bar.z<-colMeans(Z)
abline(a=(a2[1]/a2[2])*bar.z[1]+bar.z[2],
       b=-a2[1]/a2[2], lwd=1.5)
# 1st rotated axis 
abline(a=(a1[1]/a1[2])*bar.z[1]+bar.z[2],
       b=-a1[1]/a1[2],lwd=1.5)
# 2nd rotated axis
points(bar.z[1],bar.z[2],pch=21,bg="red")
lines(ellipse(x=var(Z),centre=bar.z,level=0.95),col="red",lwd=2)
```

If $\rho=2/3$:

$$\Sigma_z=\left(\begin{array}{ccc} 
\frac{2}{3} & \frac{1}{3} \\
\frac{1}{3} & \frac{2}{3}
\end{array}\right)$$

Then it would change the orientation of the ellipse because of the sign of the covariance and will be really more concentrated around the sample mean, since the determinant becomes $|\Sigma_z|=0.7$:

```{r echo=FALSE, fig.dim=c(6,3)}
library(MASS)
rho=2/3
mu_z=c(2,-3)
sigma_z=matrix(c(-2*rho+2,2*rho-1,2*rho-1,-2*rho+2),nrow = 2)
#sigma_z
Z<-mvrnorm(n=100,mu=mu_z,Sigma=sigma_z)
#Z
colnames(Z)=c("x1","x2")

plot(x2~x1, data=Z, xlab="Z1", ylab="Z2", asp=1,
     main="Plot of Z with rho=2/3", pch=16,
     xlim=c(min(x1)-1,max(x1)+1), ylim=c(min(x2)-1,max(x2)+1))                           

#round(cov(Z),3)
# sample covariance matrix
#round(cor(Z),3)
# sample correlation matrix

pca.norm<- prcomp(Z)
# Principal Component Analysis
#pca.norm
#eigen(cov(Z))



#summary(pca.norm)

library(ellipse)

a1<-pca.norm$rotation[,1];a2<-pca.norm$rotation[,2]
bar.z<-colMeans(Z)
abline(a=(a2[1]/a2[2])*bar.z[1]+bar.z[2],
       b=-a2[1]/a2[2], lwd=1.5)
# 1st rotated axis 
abline(a=(a1[1]/a1[2])*bar.z[1]+bar.z[2],
       b=-a1[1]/a1[2],lwd=1.5)
# 2nd rotated axis
points(bar.z[1],bar.z[2],pch=21,bg="red")
lines(ellipse(x=var(Z),centre=bar.z,level=0.95),col="red",lwd=2)
```
\newpage


# Exercise 3

## Point 3.1 

For this point we have to perform a Principal Component Analysis.
We have standardized data of Usair pollution and made a boxplot to spot any possible univariate outlier. Note, all these plots non-standardized are already present at point 1.2, but they were not scaled. In any case the outliers are the same. This is just a reminder of those plots.

```{r echo=FALSE, message=FALSE, warning=FALSE}
Us<-usair1
#Us
standard=scale(Us,center=T,scale=T)
#standard
boxplot(standard, main="Boxplot of Us air pollution")
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
standard.pca=prcomp(standard)
summary(standard.pca)
```

We have performed principal component analysis on standardized data ,which will give us 6 new variables,
which are linear combinations of the original ones, having as a aim a data dimensionality reduction preserving all or most of  information provided by all original data. Furthermore we presented a summary of the basic feature of these new six variables.

Now we present the standard deviation of each principal component and the coefficients of rotation of the axis given by the linear transformation performed in principal components which gives us back a sense of how original variables "load" high or low on each principal component.

```{r echo=FALSE}
standard.pca
#standard.pca$rotation
```

\newpage 

## Point 3.2

Now we plot the cumulative proportion of variance explained by the principal components, to show how
many principal components account for the majority of total variance and taking the decision to retain some accordingly, to achieve the data dimensionality we were aiming.  

```{r echo=FALSE, fig.dim=c(6,4)}
plot(summary(standard.pca)$importance[3,],xlab="Principal components",ylab="Cumulative proportion",pch=16, main="Cumulative proportion of PCs")
```

Here we produced this plot having in mind basically the same thing as before and take the decision to retain just three of the six PCs we have found. In fact, as we can see, the variance is decreasing, so the first PCs explain the majority of the variance.  

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,3.5)}
screeplot(standard.pca,type="lines",pch=16, main="Variance of PCs")

```

Now we will produce a graph representing the proportion of variance (not the variance itself) of each PC in a decreasing order , in order to see the proportion explained by each PC.  

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,4)}
p<-dim(Us)[2]

plot(1:p,summary(standard.pca)$importance[2,],type="b",main="Proportion of variance of PCs",ylab="Proportion of variance",xlab = "Principal components",pch=16)
```

Our suggestion, based on three preceding plots, is to retain just the first three PCs in 
order to get a satisfactory data reduction, while preserving the most of the information. This holds 
true because the first three PCs account for almost 85% of the total variance as we can see in the first table.  

*Note*: From the screeplot of variances we can see that the "elbow" is in the PC5. This should suggest us to take the first 4 PCs. However, as stated before, the first three PCs explain more than the 85% of the information, that is a satisfactory proportion of information and the dimensionality reduction would be important. So in front of an important dimensionality reduction we retain that to perform a satisfactory analysis we can consider the first three PCs.

\newpage

## Point 3.3

The first three principal components are a linear combination of the original variables and in the
interpretation is related to the loadings. In fact the rotation coefficient of each variable on the 
principal component gives us a hint on how the variables influence the values of the principal component.
Given that, we propose the interpretation of the first three principal components: the first one, having high loadings on human ecology, is a component which accounts for a lot of information given by the two original variables Man. and Pop.; the last two instead have high loadings on Climate factors, so they represent the information given by the original variables Neg.Temp.,Days and Precipitation. The loading of the variable Wind was really high only on PC4, but it's clear that the information present in the first three PCs is enough to continue considering it. In any case its behavior was quite regular with respect to every analysis performed. 

## Point 3.4

Now we can study the scores. The scores are the data transformed by applying the linear combination proposed by the PCs.

```{r echo=FALSE, message=FALSE, warning=FALSE}
Uspca=predict(standard.pca)[,1:3]
Uspca<-data.frame(Uspca)
kbl(summary(Uspca),booktabs = T)
```

Now we can start studying the normality, but before doing it let's see the three boxplots.

```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,3))

boxplot(Uspca[,1], main="PC1")
values<-boxplot.stats(Uspca[,1])
ind1<-which(Uspca[,1]<values$stats[1] | Uspca[,1]>values$stats[5])
ind2<-which(Uspca[,2]<values$stats[1] | Uspca[,2]>values$stats[5])
#ind2

text(rep(1,2),Uspca[,1][ind1],labels=as.character(ind1),pos=4,cex=0.75,offset=0.3)
points(rep(1,2),Uspca[,1][ind1],col="red",pch=16)

values<-boxplot.stats(Uspca[,2])
#values

boxplot(Uspca[,2],main="PC2")
text(rep(1,3),Uspca[,2][ind2],labels=as.character(ind2),pos=4,cex=0.75,offset=0.3)
points(rep(1,3),Uspca[,2][ind2],col="red",pch=16)



boxplot(Uspca[,3],main="PC3")
```


One interesting thing to note is that observation 11 is an outlier for PC1, while we recognized it also as an outlier in point 1.2 for Manuf and Pop. This confirm that their influence in this PC is important.  
Now we'll check the multivariate normality for these three PCs.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,3.5)}
d=mahalanobis(Uspca,center =colMeans(Uspca),cov=var(Uspca))
color=rep("black",41)
ind=which(sort(d)>8)  #Observations on the sorted vector
ind1=which(d>8)      #Observations on the sample
color[ind]="red"
plot(qchisq(ppoints(d),df=3),sort(d),col=color,pch=16,main="Chisq Q-Q plot of Mahalanobis distance",
     xlab="Theoretical Quantiles",ylab="Sample Quantiles")
abline(0,1,col="red")
text(qchisq(ppoints(d),df=3)[c(40,41)],sort(d)[c(40,41)],labels = as.character(ind1),pos=2,cex = 0.75,offset = 0.3)

```


We have computed the Mahalanobis distances of the values of our three principal components and produced a 
graph which compares them with quantiles of a chi squared distribution in order to check for multivariate 
normality and to check for multivariate outliers which we have highlighted in red and labeled with the 
observation they correspond to. From this we can deduce that observation 1 and 11 are also multivariate 
outliers. We detected them also in exercise 1, but this time we were not able to detect the observation 9, which does not appear from this analysis.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,3.5)}
color=rep("black",41)
ind1=which(d>8)
color[ind1]<-"red"
plot(d,main = "Mahalanobis distance",pch=16,col=color)
text(c(ind1),d[ind1],labels = ind1,pos = 4,cex=0.75,offset = 0.3)
n=41
abline(h=qchisq((n-0.5)/n,df=3),lty=2,col="red")

abline(h=qchisq((n-0.25)/n,df=3),lty=2,col="green")

abline(h=qchisq((n-0.1)/n,df=3),lty=2,col="yellow")
```

We produced a scatterplot of Mahalanobis distances together with quantile lines of chi squared distribution
which correspond to 95%,97.5% and 99% to highlight even more the observations which are multivariate
outliers.

\newpage

## Point 3.5

Now by looking at the last two PCs we can start looking for suspect observations. Recall that a suspect observation is a univariate outlier for the last PCs.  
We will first draw the two boxplot for each PCs interested.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,3.5)}
par(mfrow=c(1,2))
boxplot(predict(standard.pca)[,5], main="PC5")

last<-predict(standard.pca)[,6]
boxplot(last,main="PC6")
values<-boxplot.stats(last)
ind1<-which(last<values$stats[1] | last>values$stats[5])


text(rep(1,1),last[ind1],labels=as.character(ind1),pos=4,cex=0.75,offset=0.3)
points(rep(1,1),last[ind1],col="red",pch=16)
```

In the PC5 we couldn't find any outlier, while in PC6 there is one: score 27.  

As a further analysis we can check the bivariate plot to see if it is a multivariate outlier. We can see that it is not, since it doesn't seem to be strange for PC5.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,3.5)}
color=rep("black",41)
color[ind1]<-"red"
x<-predict(standard.pca)[,5]
y<-predict(standard.pca)[,6]
plot(y~x,pch=16,xlab="PC5",ylab="PC6",col=color,main="Scores")
```

In fact if we restrict to the axis of PC5 we can see that it is not an outlier for PC5.  

When we were looking for multivariate outliers with the original variables we checked if from the bivariate plot there could arise some interesting comment and then we compared the Mahalanobis distance. In this case we had just one possible suspect observation, that didn't behave strangely with respect to PC5, so we conclude that it's not a multivariate outlier. We can confirm this also looking at the comparison of the Mahalanobis distance with the chi-squared quantile (95%).

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,4)}
Uspca2<-data.frame(predict(standard.pca)[,5:6])
#Uspca2
S<-cov(Uspca2)
uspca2_mean<-colMeans(Uspca2)
d<-mahalanobis(Uspca2,center = uspca2_mean,cov=S)
plot(d,pch=16,main="Mahalanobis distance for PC5 and PC6",col=color)

abline(h=qchisq((n-0.5)/n,df=2),lty=2,col="red")
```

As we can see it's just a little bit over the 95% quantile.  
To conclude observation 27 is a suspect observation since it's an outlier for PC6, but it's not a multivariate outlier since from the last plot we can see it's just over the 95% quantile.    
Furthermore we can see that it was an outlier for Manuf, and as expected Manuf has an important impact on PC6. 

\newpage

## Point 3.6

During this analysis we will study what happens when we take into account the PCA with respect to the prediction of the SO2 values, that we have excluded from the beginning.  
We will associate the first 3 PCs with every city and then use them to explore their relationship with
SO2.

```{r echo=FALSE, message=FALSE, warning=FALSE}
principal_data = prcomp(usair[,2:7], scale. = TRUE)
pca_var = principal_data$x[,1:3]
usair.pca = data.frame(usair[,1],pca_var)
names(usair.pca)[1] = "SO2"
#usair.pca
library(kableExtra)

kbl(summary(usair.pca),booktabs = T)
```

 Now we can start our analysis. First of all, we will do 3 scatterplots to visualize the correlation between SO2 and each PC and we will build a linear model to see the relationship between SO2 and the first PC. Then we will calculate the R-squared, an important statistical measure which is a regression model that represents the proportion of the difference or variance in statistical terms for a dependent variable which can be explained by an independent variable or variables. In short, it determines how well data will fit the regression model. (In the following plots the title is the multiple R-squared).
 
```{r echo=FALSE, message=FALSE, warning=FALSE}
lm.PC1 = lm(SO2~PC1, usair.pca)
#summary(lm.PC1)
```
 
```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,4)}
ggplot(usair.pca, aes(x = PC1, y = SO2)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")+
  ggtitle(label= paste("R=",round(summary(lm.PC1)[[8]],5)))

lm.PC2 = lm(SO2~PC2, usair.pca)
#summary(lm.PC2) # R2 = 0.014, R2adj = -0.01129
# Plotting SO2 vs. PC2
ggplot(usair.pca, aes(x = PC2, y = SO2)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")+
  ggtitle(label= paste("R=",round(summary(lm.PC2)[[8]],5)))

# Linear model SO2 vs.PC3
lm.PC3 = lm(SO2~PC3, usair.pca)
#summary(lm.PC3) # R2 = 0.0003649, R2adj = -0.02527
# Plotting SO2 vs. PC3
ggplot(usair.pca, aes(x = PC3, y = SO2)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")+
  ggtitle(label= paste("R=",round(summary(lm.PC3)[[8]],5)))
```

 Now we know that the first PC is the one which explain the most correlation with SO2.  
 We can then build a linear model considering all the 3 PC together, knowing that the relationship with the first PC is much stronger than the others. (the linear models with PC2 and PC3 have an R2 of 0.014 and 0.0003649 respectively, so there is almost no correlation between those PCs and SO2).
 
```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,4)}
lm.pca = lm(SO2~PC1+PC2+PC3, usair.pca)
#summary(lm.pca) # R2 = 0.4182, R2adj = 0.371
# Plotting SO2 vs. PC1+PC2+PC3
ggplot(usair.pca, aes(x = PC1+PC2+PC3, y = SO2)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")+
  ggtitle(label= paste("R=",round(summary(lm.pca)[[8]],5)))

```
 
 As expected, R is a bit higher (thanks to the more information gained) than the one with only the first PC, but not in a relevant way.
 
 Now it's time to plot our PCA. We will make 3 biplots, which include both the position of each
 sample in terms of PC1, PC2 and PC3 (every plot will show a different pair each times and
 it also will show how the initial variables map onto this. We will use the ggbiplot package,
 which offers a user-friendly and pretty function to plot biplots. A biplot is a type of plot 
 that will allow you to visualize how the samples relate to one another in our PCA, so
 which samples are similar and which are different) and will simultaneously reveal
 how each variable contributes to each principal component.
 
```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,4)}
library(ggbiplot)

ggbiplot(principal_data,choices=c(1,2))

#Let's have a look at PC2 and PC3:
ggbiplot(principal_data,choices=c(2,3))

#Let's have a look at PC1 and PC3:
ggbiplot(principal_data,choices=c(1,3))
```
 
 We can see how in the first and in the third biplot 'Manuf' and 'Pop' (the two Human ecology variables)
are the ones which contribute the most to PC1, the PC that in the previous linear models has the
highest R-squared. We can ignore the second biplot because PC2 and PC3 have almost no correlation with SO2. In fact they seem to be more affected from the climate variables.
 Furthermore, in the next exercise we can expect that the Human ecology variables  have a higher
 R-squared than the Climate variables.
 
 \newpage
 
## Point 3.7

In this last point we have to investigate about the use of multiple linear regression on the air pollution data using the human ecology and climate variables.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,4)}
lm_normal_he = lm(SO2~Manuf+Pop, usair)
#summary(lm_normal_he) # R2 = 0.586 and R2_adj = 0.5645
# Let's see the plot
ggplot(usair, aes(x = Manuf+Pop, y = SO2)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")+
  ggtitle(label= paste("R=",round(summary(lm_normal_he)[[8]],5)))

lm_normal_cv = lm(SO2~Neg.Temp+Wind+Precip+Days, usair)
#summary(lm_normal_cv) # R2 = 0.255 and R2_adj = 0.1722
# Let's see the plot
ggplot(usair, aes(x = Neg.Temp+Wind+Precip+Days, y = SO2)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")+
  ggtitle(label= paste("R=",round(summary(lm_normal_cv)[[8]],5)))
```

As expected, the Human ecology variables have a stronger correlation with SO2 than the Climate variables.
We can confirm this stronger relationship with a further analysis.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.dim=c(6,4)}
lm_normal = lm(SO2~., data = usair)
# Let's see the plot
ggplot(usair, aes(x = Manuf+Pop+Neg.Temp+Wind+Precip+Days, y = SO2)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")+
    ggtitle(label= paste("R=",round(summary(lm_normal)[[8]],5)))



summary(lm_normal)[c(4,8,9)]

```


If the p-value for a variable is less than our significance level (let’s presume 0.025), our sample data provide enough evidence to reject the null hypothesis for the entire population. In our case the data favor the hypothesis that there is a non-zero correlation.
According to the p-values of the predictors, ‘Manuf’ and ‘Pop’, are the most relevant (<0.025) to predict SO2. It confirms our previous analysis in the exercise 3.6.
